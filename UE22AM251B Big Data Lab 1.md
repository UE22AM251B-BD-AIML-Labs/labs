# UE22AM251B Big Data Lab 1
Basics of Hadoop Distributed File Systems (HDFS), working with large text files

## Assignment Objectives and Outcomes

The objective of this acssignment is to deepen your understanding of the practical applications of Hadoop Distributed File System (HDFS) configuration in real-world scenarios. By exploring different use cases and industries, you will gain hands-on experience in tailoring HDFS configurations to meet specific requirements, optimizing storage efficiency, and ensuring the reliable and efficient storage and processing of large-scale data.



## Software/Languages to be used
1. Python ```3.10.x```
2. Hadoop ```3.3.6```

## Submission Deadline

## Environment Setup

1. Download [this hadoop installation script](https://drive.google.com/file/d/1c6m0jkSAH2a_iJeWYMXAkbvuWPF7sU_y/view?usp=sharing) as a first step to setup Hadoop on your system.
2. Run the following commands
```
chmod +x *.sh    #Giving executable permissions to all shell scripts
dos2unix *.sh    #Important for converting scripts from CRLF to LF
source hadoop.sh  #Execute the script
```

3. Congratulations, Hadoop is set-up!
 
## Input Files

Download this [100 MB placeholder text file](https://drive.google.com/file/d/1HegUQa2Zk75elmmCnLWJ3T_DjNJzj0P9/view?usp=sharing).

Download this [Reboot Hadoop file](https://drive.google.com/file/d/1X2hoIhDlfjDSbWCFB3y3FO2DyS9Zq4E4/view?usp=sharing)

## Tasks Background

Hadoop Distributed File System (HDFS) is a key component of the Hadoop ecosystem and is widely used in various real-life scenarios for large-scale data storage and processing. Here are several areas where HDFS configuration is crucial in real-world applications:

### Big Data Analytics:

Scenario: Organizations dealing with massive volumes of data, such as e-commerce platforms, social media networks, and financial institutions, leverage HDFS for storing and processing large datasets.
HDFS Configuration Impact: Proper tuning of HDFS configurations, including block size, replication factor, and storage policies, significantly influences the performance and efficiency of big data analytics.

### Data Warehousing:

Scenario: Enterprises use HDFS to store and manage data for data warehousing solutions, allowing them to perform complex queries and analysis on structured and semi-structured data.
HDFS Configuration Impact: Adjusting configurations related to compression, storage policies, and tiered storage ensures optimal utilization of resources and supports the specific requirements of data warehousing workloads.

### Log Processing and Aggregation:

Scenario: Companies dealing with log data, such as those in cybersecurity or IT operations, often use HDFS for storing and processing log files generated by various systems.
HDFS Configuration Impact: Configuring HDFS to handle a high volume of small files efficiently, setting replication factors based on fault tolerance needs, and implementing storage policies for archiving or frequently accessed logs are crucial

### Machine Learning and AI:

Scenario: Organizations working on machine learning and artificial intelligence applications use HDFS to store and manage training datasets, models, and intermediate results.
HDFS Configuration Impact: Optimizing block size, replication, and storage policies can enhance the training and inference performance of machine learning algorithms.



... and so on.


## Task Specifications

You will be working on tasks playing around with three configuration metrics:
1) Replication Factor
    
> The replication factor is a crucial configuration parameter in HDFS that determines the number of copies (replicas) of each data block stored in the system.
> 
> Advantages of Increasing Replication Factor:
> 1. Improved Fault Tolerance
> 2. Enhanced Read Performance
> 3. Increased Availability
> 
> Disadvantages of Increasing Replication Factor:
> 1. Increased Storage Overhead
> 2. Impact on Write Performance
> 3. Network Overhead


3) Block Size
> The block size is a critical configuration parameter in HDFS that determines the size of individual data blocks. Altering the block size can have significant implications for HDFS performance.
> 
> Advantages of Increasing Block Size:
> 1. Reduced Metadata Overhead
> 2. Enhanced Throughput for Large Files
> 3. Reduced Network Overhead
> 
> Disadvantages of Increasing Block Size:
> 1. Increased Storage Fragmentation
> 2. Reduced Parallelism for Small Files
> 3. Impact on Job Scheduling

5) Disk addition to Datanodes
> Hadoop Distributed File System (HDFS) relies on distributed storage across multiple DataNodes to provide fault tolerance and scalability.
> 
> Advantages of Adding Disks to DataNodes:
> 1. Increased Storage Capacity
> 2. Enhanced Data Distribution
> 3. Reduced Disk I/O Contention
> 
> Disadvantages of Adding Disks to DataNodes:
> 1. Increased Management Overhead
> 2. Potential for Uneven Disk Usage
> 3. Impact on Network Bandwidth

### Prepare for first run
1. Restart all hadoop services and format all hdfs namenode data and check jps for the presence of the following 6 processes.

```
chmod +x rebootHadoop.sh

source rebootHadoop.sh
```

Ideal JPS output:

```
NameNode
DataNode
NodeManager
SecondaryNameNode
ResourceManager
Jps     
```

### Task 1 - Replication Factor

1. Check the current replication factor setup by dafault in your hadoop system
    ```
    hdfs getconf -confKey dfs.replication
    
    #Default should be 1, if no prior modifications have been done, signalling there is no replication being done currently.
    ```

2. Compare the performance of reading and writing data with the initial replication factor and write output into files

    ```
   hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB -resFile SRN_write_rf1.txt

    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 10MB -resFile SRN_read_rf1.txt
    ```
    
3. Create a directory in HDFS storage and then store the given file.

    ```
    hdfs dfs -mkdir /L1

    hdfs dfs -put 100MB.txt /L1/ 
    ```

4. Change the replication factor to 3.
    ```
    hdfs dfs -setrep -R 3 /L1/100MB.txt
    
    # R Flag refers to set the replication factor being set across all the files recursively in a folder (isn't applicable in this case since there's no sub-directories)
    ```
    
5. Check the replication factor of the file for sanity-check.

    ```
    hdfs dfs -stat "%r" /L1/100MB.txt
    ```

6. Compare the performance of reading and writing data with the initial replication factor.

    ```
    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB -resFile SRN_write_rf2.txt

    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 10MB -resFile SRN_read_rf2.txt 
    ```
    

7. Change the replication factor to 10.
    ```
    hdfs dfs -setrep -R 10 /L1/100MB.txt
    
    # R Flag refers to set the replication factor being set across all the files recursively in a folder (isn't applicable in this case since there's no sub-directories)
    ```
    
8. Run Command 5 again for sanity check.

9. Compare the performance of reading and writing data with the initial replication factor.

    ```
    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB -resFile SRN_write_rf3.txt

    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 10MB -resFile SRN_read_rf3.txt 
    ```
    
10. Set the replication factor to 1 again.
    
### Task 2 - Understanding Partitions

1. Check the number of blocks and their distribution across DataNodes using the following command:
    ```
    hdfs fsck /L1/100MB.txt -files -blocks -locations
    ```
    
2. Run the following command to check the current block size configuration:

    ```
    hdfs dfs -stat %o /L1/100MB.txt
    ```
3. Change the block size of a file to 128 in HDFS and observe the impact on the number of blocks. Use the command:

    ```
    hdfs dfs -D dfs.blocksize=128 -put 100MB.txt /L1

    ```

    
4. Compare the performance of reading and writing data.

    ```
    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB -resFile SRN_write_part1.txt

    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 10MB -resFile SRN_read_part1.txt
    ```

5. Remove the file.
    ```
    hdfs dfs -rm -r /L1/100MB.txt
    ```

    
6. Change the block size of a file to 256 in HDFS and observe the impact on the number of blocks. Use the command:

    ```
    hdfs dfs -D dfs.blocksize=256 -put 100MB.txt /L1
    
    ```

7. Run command 2 again to check if the block size has changed successfully.
    
8. Compare the performance of reading and writing data with the altered block size.

    ```
    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB -resFile SRN_write_part2.txt

    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 10MB -resFile SRN_read_part2.txt
    ```

9. Change the block size of a file to 512 in HDFS and observe the impact on the number of blocks. Use the command:

    ```
    hdfs dfs -D dfs.blocksize=512 -put 100MB.txt /L1
    
    ```
    
10. Run command 2 again to check if the block size has changed successfully.

11. Compare the performance of reading and writing data with the altered block size.

    ```
    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB -resFile SRN_write_part3.txt

    hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 10MB -resFile SRN_read_part3.txt
    ```
    
## Task Deliverables

After completing all the steps in Task-1 and Task-2, you will have 12 txt files with following naming conventions - 

```
SRN_write_rf1.txt
SRN_read_rf1.txt
SRN_write_rf2.txt
SRN_read_rf2.txt
SRN_write_rf3.txt
SRN_read_rf3.txt
SRN_write_part1.txt
SRN_read_part1.txt
SRN_write_part2.txt
SRN_read_part2.txt
SRN_write_part3.txt
SRN_read_part3.txt

```

Please save all these files in a folder with the following naming convention- 

```
Section_SRN_Lab1

```

Zip this folder and submit it in the form link mentioned in your assessment announcement.


## HDFS Operations

The HDFS supports all file operations and is greatly similar to the file system commands available
on Linux.

You can access HDFS on command line using `hdfs dfs` and use the `-` prefix before the file
system command to execute general Linux file system commands.

### Loading a file into HDFS

A file can be loaded into HDFS using the following command.

`hdfs dfs -put path_to_file /hdfs_directory_path`

### Listing files on HDFS

Files can be listed on HDFS using

```hdfs dfs -ls /hdfs_directory_path```

Similarly, HDFS also supports `-mkdir` ,` -rm `and more.

### Displaying contents of files on HDFS

File contents can be displayed using

`hdfs dfs -ls /hdfs_directory_path`

## Running a MapReduce Job

A MapReduce job can be run using the following command

```
hadoop jar path-to-streaming-jar-file \
-input path_to_input_folder_on_hdfs \
-output path_to_output_folder_on_hdfs \
-mapper absolute_path_to_mapper.py command_line_arguments \
-reducer absolute_path_to_reducer.py command_line_arguments
```

```path-to-streaming-jar-file``` is ```$HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar``` if you have followed the previous lab's tutorial on installing Hadoop.